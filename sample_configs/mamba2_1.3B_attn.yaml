base_model: /path/to/a/base/model/dir
model_name: mamba2_1.3B_attn
n_layer: 48
d_model: 2048
ssm_cfg:
  layer: Mamba2
attn_layer_idx: [8, 24, 40]
attn_cfg:
  causal: true
  d_conv: 4
  head_dim: 128
  num_heads: 30
  out_proj_bias: false
  qkv_proj_bias: false
  rotary_emb_dim: 64
chunk_size: 256
batch_size: 80
gradient_accumulation_steps: 1
max_grad_norm: 1.0
max_steps: -1
num_train_epochs: 1
logging_steps: 25
warmup_steps: 300
save_steps: 100
save_total_limit: 7
learning_rate: 0.005
